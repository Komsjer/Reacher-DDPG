{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DDPG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'C:/Users/Gebruiker/Documents/Coding testr/Reacher/multiAgent/Reacher.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=PATH, no_graphics = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epocs = 250 #number of episodes\n",
    "epoc_length = 1000 #steps per episode\n",
    "rewards     = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoc:   0 reward: 0.74\n",
      "epoc:   1 reward: 1.09\n",
      "epoc:   2 reward: 2.50\n",
      "epoc:   3 reward: 3.65\n",
      "epoc:   4 reward: 4.26\n",
      "epoc:   5 reward: 5.57\n",
      "epoc:   6 reward: 5.54\n",
      "epoc:   7 reward: 5.61\n",
      "epoc:   8 reward: 6.49\n",
      "epoc:   9 reward: 6.31\n",
      "epoc:  10 reward: 7.16\n",
      "epoc:  11 reward: 9.18\n",
      "epoc:  12 reward: 7.13\n",
      "epoc:  13 reward: 6.26\n",
      "epoc:  14 reward: 6.79\n",
      "epoc:  15 reward: 7.10\n",
      "epoc:  16 reward: 7.63\n",
      "epoc:  17 reward: 8.52\n",
      "epoc:  18 reward: 6.96\n",
      "epoc:  19 reward: 7.48\n",
      "epoc:  20 reward: 6.86\n",
      "epoc:  21 reward: 7.14\n",
      "epoc:  22 reward: 7.64\n",
      "epoc:  23 reward: 6.52\n",
      "epoc:  24 reward: 7.15\n",
      "epoc:  25 reward: 7.83\n",
      "epoc:  26 reward: 7.16\n",
      "epoc:  27 reward: 7.40\n",
      "epoc:  28 reward: 7.30\n",
      "epoc:  29 reward: 6.66\n",
      "epoc:  30 reward: 6.76\n",
      "epoc:  31 reward: 5.92\n",
      "epoc:  32 reward: 7.98\n",
      "epoc:  33 reward: 8.32\n",
      "epoc:  34 reward: 9.12\n",
      "epoc:  35 reward: 8.47\n",
      "epoc:  36 reward: 8.29\n",
      "epoc:  37 reward: 7.49\n",
      "epoc:  38 reward: 8.95\n",
      "epoc:  39 reward: 9.84\n",
      "epoc:  40 reward: 9.68\n",
      "epoc:  41 reward: 9.90\n",
      "epoc:  42 reward: 9.97\n",
      "epoc:  43 reward: 9.90\n",
      "epoc:  44 reward: 11.83\n",
      "epoc:  45 reward: 12.10\n",
      "epoc:  46 reward: 14.18\n",
      "epoc:  47 reward: 12.88\n",
      "epoc:  48 reward: 10.41\n",
      "epoc:  49 reward: 11.39\n",
      "epoc:  50 reward: 13.00\n",
      "epoc:  51 reward: 14.81\n",
      "epoc:  52 reward: 15.49\n",
      "epoc:  53 reward: 12.32\n",
      "epoc:  54 reward: 15.99\n",
      "epoc:  55 reward: 13.66\n",
      "epoc:  56 reward: 11.60\n",
      "epoc:  57 reward: 12.20\n",
      "epoc:  58 reward: 13.94\n",
      "epoc:  59 reward: 15.21\n",
      "epoc:  60 reward: 16.18\n",
      "epoc:  61 reward: 14.70\n",
      "epoc:  62 reward: 13.23\n",
      "epoc:  63 reward: 11.70\n",
      "epoc:  64 reward: 13.47\n",
      "epoc:  65 reward: 12.92\n",
      "epoc:  66 reward: 14.77\n",
      "epoc:  67 reward: 13.04\n",
      "epoc:  68 reward: 14.90\n",
      "epoc:  69 reward: 12.14\n",
      "epoc:  70 reward: 13.56\n",
      "epoc:  71 reward: 15.39\n",
      "epoc:  72 reward: 16.20\n",
      "epoc:  73 reward: 16.00\n",
      "epoc:  74 reward: 14.17\n",
      "epoc:  75 reward: 14.98\n",
      "epoc:  76 reward: 15.12\n",
      "epoc:  77 reward: 16.69\n",
      "epoc:  78 reward: 16.82\n",
      "epoc:  79 reward: 19.79\n",
      "epoc:  80 reward: 22.77\n",
      "epoc:  81 reward: 21.10\n",
      "epoc:  82 reward: 20.18\n",
      "epoc:  83 reward: 21.17\n",
      "epoc:  84 reward: 23.68\n",
      "epoc:  85 reward: 22.04\n",
      "epoc:  86 reward: 24.24\n",
      "epoc:  87 reward: 23.96\n",
      "epoc:  88 reward: 23.77\n",
      "epoc:  89 reward: 24.22\n",
      "epoc:  90 reward: 26.63\n",
      "epoc:  91 reward: 22.82\n",
      "epoc:  92 reward: 23.69\n",
      "epoc:  93 reward: 27.98\n",
      "epoc:  94 reward: 30.52\n",
      "epoc:  95 reward: 25.60\n",
      "epoc:  96 reward: 28.64\n",
      "epoc:  97 reward: 33.00\n"
     ]
    }
   ],
   "source": [
    "for epoc in range(epocs):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations\n",
    "    cumulative_state = np.concatenate((state, state), axis = 1)\n",
    "    epoc_reward = 0\n",
    "\n",
    "    for step in range(epoc_length):\n",
    "        action = agent.get_action(cumulative_state)\n",
    "        action = np.clip(action, -1, 1)  \n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations\n",
    "        reward = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        \n",
    "        next_cumulative_state = np.concatenate((next_state, state), axis = 1)\n",
    "        agent.add_replay(cumulative_state, action, reward, next_cumulative_state, done)\n",
    "        \n",
    "        agent.learning_step()\n",
    "        \n",
    "        state = next_state\n",
    "        cumulative_state = next_cumulative_state\n",
    "        epoc_reward += sum(reward)/num_agents\n",
    "        \n",
    "    print (\"epoc: {0:3} reward: {1:2.2f}\".format(epoc,epoc_reward))\n",
    "                 \n",
    "    rewards.append(epoc_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.ylabel('reward')\n",
    "plt.xlabel('epoc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
